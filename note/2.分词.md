# jieba
## 安装
pip install jieba

git clone https://github.com/fxsjy/jieba.git
python setup.py install
## 分词算法
- 基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）；
- 基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词；
- 对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。

## api参数
- jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。
- jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细

## 精确分词 
精确模式试图将句子最精确地切开

``` python
import jieba
content = "现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。"

segs_1 = jieba.cut(content, cut_all=False) 
print("/".join(segs_1))
```
> 输出：现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。

## 全模式 
把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义

``` python
segs_3 = jieba.cut(content, cut_all=True)
print("/".join(segs_3))
```
> 输出： 现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功//

## 搜索引擎模式 
在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。

``` stylus
segs_4 = jieba.cut_for_search(content)
print("/".join(segs_4))
```
> 输出： 如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。

## lcut
生成 list

``` stylus
segs_5 = jieba.lcut(content)
print(segs_5)
```

> 输出： ['现如今', '，', '机器', '学习', '和', '深度', '学习', '带动', '人工智能', '飞速', '的', '发展', '，', '并', '在', '图片', '处理', '、', '语音', '识别', '领域', '取得', '巨大成功', '。']

## 获取词性  
jieba.posseg 模块实现词性标注

``` stylus
import jieba.posseg as psg
print([(x.word, x.flag) for x in psg.lcut(content)])
```

> 输出：[('现如今', 't'), ('，', 'x'), ('机器', 'n'), ('学习', 'v'), ('和', 'c'), ('深度', 'ns'), ('学习', 'v'), ('带动', 'v'), ('人工智能', 'n'), ('飞速', 'n'), ('的', 'uj'), ('发展', 'vn'), ('，', 'x'), ('并', 'c'), ('在', 'p'), ('图片', 'n'), ('处理', 'v'), ('、', 'x'), ('语音', 'n'), ('识别', 'v'), ('领域', 'n'), ('取得', 'v'), ('巨大成功', 'nr'), ('。', 'x')]







## 并行分词 
为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果,默认分词器 jieba.dt 和 jieba.posseg.dt暂不支持 Windows。

``` mipsasm
jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。
jieba.disable_parallel() # 关闭并行分词模式 。
```


## Counter
获取分词结果中词列表的 top n

``` python
from collections import Counter
top5 = Counter(segs_5).most_common(5)
print(top5)
```
> 输出：[('，', 2), ('学习', 2), ('现如今', 1), ('机器', 1), ('和', 1)]


## 自定义添加词和字典

``` bash
txt = "铁甲网是中国最大的工程机械交易平台。"
jieba.add_word("铁甲网")
print(jieba.lcut(txt))

jieba.load_userdict('user_dict.txt') # 添加词典
print(jieba.lcut(txt))
```
> 输出：['铁甲网', '是', '中国', '最大', '的', '工程机械', '交易平台', '。']



# hanlp 
## 安装
- pip install pyhanlp 如报错building '_jpype' extensionerror: Microsoft Visual C++ 14.0 is required，则conda install -c conda-forge jpype1;pip install pyhanlp
- 如ValueError: 配置错误: 数据包/pyhanlp/static\data 不存在，请修改配置文件中的root，则到https://github.com/hankcs/HanLP/releases下载data-for-1.7.2.zip下载后放入f:/anaconda3/envs/learn/lib/site-packages/pyhanlp/static\data
- 安装jdk环境 
- hanlp segment 交互分词模式
![a微信截图_20191121171036](https://www.github.com/OneJane/blog/raw/master/小书匠/a微信截图_20191121171036.png)
- hanlp serve 内置http服务器 http://localhost:8765


## 分词

``` jboss-cli
from pyhanlp import *
content = "现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。"
print(HanLP.segment(content))
```
> 输出：[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w]


## 自定义词典分词

``` python
txt = "铁甲网是中国最大的工程机械交易平台。"
CustomDictionary.add("铁甲网")
CustomDictionary.insert("工程机械", "nz 1024")
CustomDictionary.add("交易平台", "nz 1024 n 1")
print(HanLP.segment(txt))
```
> 输出： [铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w]


> 以上两种工具可以做关键词提取、自动摘要、依存句法分析、情感分析等

