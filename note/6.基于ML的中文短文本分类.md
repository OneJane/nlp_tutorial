**对每一条输入数据，判断事情的主体是谁**
- - 语料加载
- 分词
- 去停用词
- 抽取词向量特征
- 分别进行算法建模和模型训练
- 评估、计算 AUC 值
- 模型对比
![enter description here](https://www.github.com/OneJane/blog/raw/master/小书匠/1574394521914.png)
# 语料加载

``` stylus
import random
import jieba
import pandas as pd
#加载停用词
stopwords=pd.read_csv('stopwords.txt',index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')
stopwords=stopwords['stopword'].values
#加载语料
laogong_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
laopo_df = pd.read_csv('beilaogongda.csv', encoding='utf-8', sep=',')
erzi_df = pd.read_csv('beierzida.csv', encoding='utf-8', sep=',')
nver_df = pd.read_csv('beinverda.csv', encoding='utf-8', sep=',')
#删除语料的nan行
laogong_df.dropna(inplace=True)
laopo_df.dropna(inplace=True)
erzi_df.dropna(inplace=True)
nver_df.dropna(inplace=True)
#提取要分词的 content 列转换为 list 列表
laogong = laogong_df.segment.values.tolist()
laopo = laopo_df.segment.values.tolist()
erzi = erzi_df.segment.values.tolist()
nver = nver_df.segment.values.tolist()
```
# 分词和去停用词

``` 
#定义分词、去停用词和批量打标签的函数
#参数content_lines即为语料列表 上面转换的list
#参数sentences是先定义的 list，用来存储分词并打标签后的结果
#参数category 是类型标签
def preprocess_text(content_lines, sentences, category):
    for line in content_lines:
        try:
            segs=jieba.lcut(line)
            segs = [v for v in segs if not str(v).isdigit()]#去数字
            segs = list(filter(lambda x:x.strip(), segs))   #去左右空格
            segs = list(filter(lambda x:len(x)>1, segs)) #长度为1的字符
            segs = list(filter(lambda x:x not in stopwords, segs)) #去掉停用词
            sentences.append((" ".join(segs), category))# 打标签
        except Exception:
            print(line)
            continue 
			
# 调用函数、生成训练数据，根据我提供的司法语料数据，分为报警人被老公打，报警人被老婆打，报警人被儿子打，报警人被女儿打，标签分别为0、1、2、3	
sentences = []
preprocess_text(laogong, sentences,0)
preprocess_text(laopo, sentences, 1)
preprocess_text(erzi, sentences, 2)
preprocess_text(nver, sentences, 3)

# 将得到的数据集打散，生成更可靠的训练集分布，避免同类数据分布不均匀
random.shuffle(sentences)

# 控制台输出前10条数据
for sentence in sentences[:10]:
    print(sentence[0], sentence[1])  #下标0是词列表，1是标签
```
![enter description here](https://www.github.com/OneJane/blog/raw/master/小书匠/1574395012948.png)
# 抽取词向量特征

``` nix
# 抽取特征，我们定义文本抽取词袋模型特征
from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer(
    analyzer='word', # tokenise by character ngrams
    max_features=4000,  # keep the most common 1000 ngrams
)
# 把语料数据切分，用 sk-learn 对数据切分，分成训练集和测试集
from sklearn.model_selection import train_test_split
x, y = zip(*sentences)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1256)
# 把训练数据转换为词袋模型
vec.fit(x_train)
```
# 分别进行算法建模和模型训练

``` 
# 定义朴素贝叶斯模型，然后对训练集进行模型训练，直接使用 sklearn 中的 MultinomialNB
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)
```

# 评估、计算 AUC 值

``` lisp
# 评分为 0.647331786543。
print(classifier.score(vec.transform(x_test), y_test))

# 测试集的预测
pre = classifier.predict(vec.transform(x_test))
```

# 模型对比
改变特征向量模型和训练模型对结果有什么变化。
``` vala 
## 改变特征向量模型:把特征做得更强一点，尝试加入抽取 2-gram 和 3-gram 的统计特征，把词库的量放大一点
from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer(
    analyzer='word', # tokenise by character ngrams
    ngram_range=(1,4),  # use ngrams of size 1 and 2
    max_features=20000,  # keep the most common 1000 ngrams
)
vec.fit(x_train)
#用朴素贝叶斯算法进行模型训练
classifier = MultinomialNB()
classifier.fit(vec.transform(x_train), y_train)
#对结果进行评分 结果评分为：0.649651972158
print(classifier.score(vec.transform(x_test), y_test))

## SVM 训练
from sklearn.svm import SVC
svm = SVC(kernel='linear')
svm.fit(vec.transform(x_train), y_train)
print(svm.score(vec.transform(x_test), y_test))
	
## 使用决策树、随机森林、XGBoost、神经网络
import xgboost as xgb  
from sklearn.model_selection import StratifiedKFold  
import numpy as np
# xgb矩阵赋值  
xgb_train = xgb.DMatrix(vec.transform(x_train), label=y_train)  
xgb_test = xgb.DMatrix(vec.transform(x_test)) 
```
在 XGBoost 中，下面主要是调参指标，可以根据参数进行调参

``` clean
params = {  
        'booster': 'gbtree',     #使用gbtree
        'objective': 'multi:softmax',  # 多分类的问题、  
        # 'objective': 'multi:softprob',   # 多分类概率  
        #'objective': 'binary:logistic',  #二分类
        'eval_metric': 'merror',   #logloss
        'num_class': 4,  # 类别数，与 multisoftmax 并用  
        'gamma': 0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。  
        'max_depth': 8,  # 构建树的深度，越大越容易过拟合  
        'alpha': 0,   # L1正则化系数  
        'lambda': 10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。  
        'subsample': 0.7,  # 随机采样训练样本  
        'colsample_bytree': 0.5,  # 生成树时进行的列采样  
        'min_child_weight': 3,  
        # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言  
        # 假设 h 在 0.01 附近，min_child_weight 为 1 叶子节点中最少需要包含 100 个样本。  
        'silent': 0,  # 设置成1则没有运行信息输出，最好是设置为0.  
        'eta': 0.03,  # 如同学习率  
        'seed': 1000,  
        'nthread': -1,  # cpu 线程数  
        'missing': 1 
    }  
```

